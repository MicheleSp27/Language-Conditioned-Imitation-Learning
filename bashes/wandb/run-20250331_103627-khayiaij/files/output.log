Found 4 GPU devices, using 20 parallel workers for evaluating 160 total trajectories
Renderer is using size (200, 360)
---- Testing model test_any_task.TransformerNetwork ----
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 351, in __getattr__
    return self._get_impl(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 480, in _get_node
    raise ConfigKeyError(f"Missing key {key!s}")
omegaconf.errors.ConfigKeyError: Missing key train_cfg
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/multiprocessing/pool.py", line 51, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "/raid/home/frosa_Loc/Language-Conditioned-Imitation-Learning/test/multi_task_test/test_any_task.py", line 265, in _proc
    return_rollout = rollout_imitation(model,
  File "/raid/home/frosa_Loc/Language-Conditioned-Imitation-Learning/test/multi_task_test/test_any_task.py", line 163, in rollout_imitation
    T_context = config.train_cfg.dataset.get('T_context', None)
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 355, in __getattr__
    self._format_and_raise(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 351, in __getattr__
    return self._get_impl(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/site-packages/omegaconf/dictconfig.py", line 480, in _get_node
    raise ConfigKeyError(f"Missing key {key!s}")
omegaconf.errors.ConfigAttributeError: Missing key train_cfg
    full_key: train_cfg
    object_type=dict
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/raid/home/frosa_Loc/Language-Conditioned-Imitation-Learning/test/multi_task_test/test_any_task.py", line 627, in <module>
    task_success_flags = p.starmap(f, seeds)
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/multiprocessing/pool.py", line 372, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/raid/home/frosa_Loc/conda/envs/multi_task_lfd/lib/python3.9/multiprocessing/pool.py", line 771, in get
    raise self._value
omegaconf.errors.ConfigAttributeError: Missing key train_cfg
    full_key: train_cfg
    object_type=dict